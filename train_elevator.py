#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”µæ¢¯æŒ‰é’®æ£€æµ‹æ¨¡å‹è®­ç»ƒè„šæœ¬

é›Œå°é¬¼ç‰¹åˆ¶ç‰ˆæœ¬ ğŸ€
ä¸“é—¨ç”¨äºè®­ç»ƒç”µæ¢¯æŒ‰é’®æ£€æµ‹æ¨¡å‹
"""

import os
import sys
from pathlib import Path
from typing import Optional
import yaml
import argparse

try:
    from ultralytics import YOLO
    import torch
except ImportError as e:
    print(f"âŒ å¯¼å…¥åº“å¤±è´¥: {e}")
    print("ğŸ“¦ è¯·å®‰è£…å¿…è¦çš„ä¾èµ–: pip install ultralytics torch")
    sys.exit(1)


def create_dataset_config(dataset_dir: str, output_file: str = "elevator_data.yaml") -> str:
    """
    åˆ›å»ºæ•°æ®é›†é…ç½®æ–‡ä»¶
    
    Args:
        dataset_dir: æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„
        output_file: è¾“å‡ºé…ç½®æ–‡ä»¶å
        
    Returns:
        é…ç½®æ–‡ä»¶è·¯å¾„
    """
    # ç”µæ¢¯æŒ‰é’®ç±»åˆ«ï¼ˆæ ¹æ®READMEæè¿°ï¼Œæœ‰363ä¸ªç±»åˆ«ï¼‰
    # è¿™é‡Œæä¾›ä¸€äº›å¸¸è§çš„ç”µæ¢¯æŒ‰é’®ç±»åˆ«ä½œä¸ºç¤ºä¾‹
    elevator_classes = [
        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',  # æ•°å­—æŒ‰é’®
        'B', 'B1', 'B2', 'B3',  # åœ°ä¸‹å±‚
        'G', 'L',  # å¤§å…å±‚
        'open', 'close',  # å¼€å…³é—¨
        'up', 'down',  # ä¸Šä¸‹
        'alarm', 'emergency',  # æŠ¥è­¦ç´§æ€¥
        'call', 'help',  # å‘¼å«å¸®åŠ©
        'door_open', 'door_close',  # é—¨æ§åˆ¶
        'stop', 'start',  # åœæ­¢å¼€å§‹
        'fan', 'light',  # é£æ‰‡ç¯å…‰
        'A', 'C', 'D', 'E', 'F', 'H', 'M', 'P', 'R', 'S', 'T',  # å­—æ¯æŒ‰é’®
        # ... æ›´å¤šç±»åˆ«å¯ä»¥æ ¹æ®å®é™…æ•°æ®é›†æ·»åŠ 
    ]
    
    # åˆ›å»ºé…ç½®æ–‡ä»¶å†…å®¹
    config = {
        'path': dataset_dir,  # æ•°æ®é›†æ ¹ç›®å½•
        'train': 'train/images',  # è®­ç»ƒå›¾ç‰‡è·¯å¾„ï¼ˆç›¸å¯¹äºpathï¼‰
        'val': 'val/images',    # éªŒè¯å›¾ç‰‡è·¯å¾„ï¼ˆç›¸å¯¹äºpathï¼‰
        'test': 'test/images',  # æµ‹è¯•å›¾ç‰‡è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        'nc': len(elevator_classes),  # ç±»åˆ«æ•°é‡
        'names': elevator_classes  # ç±»åˆ«åç§°åˆ—è¡¨
    }
    
    # ä¿å­˜é…ç½®æ–‡ä»¶
    config_path = os.path.join(dataset_dir, output_file)
    with open(config_path, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
    
    print(f"âœ… æ•°æ®é›†é…ç½®æ–‡ä»¶å·²åˆ›å»º: {config_path}")
    return config_path


def create_dataset_structure(base_dir: str) -> None:
    """
    åˆ›å»ºYOLOæ ¼å¼çš„æ•°æ®é›†ç›®å½•ç»“æ„
    
    Args:
        base_dir: æ•°æ®é›†æ ¹ç›®å½•
    """
    # åˆ›å»ºç›®å½•ç»“æ„
    dirs_to_create = [
        'train/images',    # è®­ç»ƒå›¾ç‰‡
        'train/labels',    # è®­ç»ƒæ ‡ç­¾
        'val/images',      # éªŒè¯å›¾ç‰‡  
        'val/labels',      # éªŒè¯æ ‡ç­¾
        'test/images',     # æµ‹è¯•å›¾ç‰‡ï¼ˆå¯é€‰ï¼‰
        'test/labels',     # æµ‹è¯•æ ‡ç­¾ï¼ˆå¯é€‰ï¼‰
    ]
    
    for dir_path in dirs_to_create:
        full_path = os.path.join(base_dir, dir_path)
        os.makedirs(full_path, exist_ok=True)
        print(f"ğŸ“ åˆ›å»ºç›®å½•: {full_path}")
    
    # åˆ›å»ºè¯´æ˜æ–‡ä»¶
    readme_content = """# ç”µæ¢¯æŒ‰é’®æ•°æ®é›†ç»“æ„è¯´æ˜ ğŸ€

## ç›®å½•ç»“æ„
```
elevator_dataset/
â”œâ”€â”€ elevator_data.yaml          # æ•°æ®é›†é…ç½®æ–‡ä»¶
â”œâ”€â”€ train/                      # è®­ç»ƒé›†
â”‚   â”œâ”€â”€ images/                 # è®­ç»ƒå›¾ç‰‡ (.jpg, .png, .jpeg)
â”‚   â””â”€â”€ labels/                 # è®­ç»ƒæ ‡ç­¾ (.txt)
â”œâ”€â”€ val/                        # éªŒè¯é›†
â”‚   â”œâ”€â”€ images/                 # éªŒè¯å›¾ç‰‡
â”‚   â””â”€â”€ labels/                 # éªŒè¯æ ‡ç­¾
â””â”€â”€ test/                       # æµ‹è¯•é›†ï¼ˆå¯é€‰ï¼‰
    â”œâ”€â”€ images/                 # æµ‹è¯•å›¾ç‰‡
    â””â”€â”€ labels/                 # æµ‹è¯•æ ‡ç­¾

## æ ‡ç­¾æ ¼å¼
æ¯ä¸ªå›¾ç‰‡å¯¹åº”ä¸€ä¸ªåŒåçš„.txtæ–‡ä»¶ï¼Œæ ¼å¼ä¸ºï¼š
```
class_id x_center y_center width height
```

å…¶ä¸­ï¼š
- class_id: ç±»åˆ«IDï¼ˆä»0å¼€å§‹ï¼‰
- x_center, y_center: è¾¹ç•Œæ¡†ä¸­å¿ƒç‚¹åæ ‡ï¼ˆç›¸å¯¹äºå›¾ç‰‡å°ºå¯¸ï¼Œ0-1ä¹‹é—´ï¼‰
- width, height: è¾¹ç•Œæ¡†å®½é«˜ï¼ˆç›¸å¯¹äºå›¾ç‰‡å°ºå¯¸ï¼Œ0-1ä¹‹é—´ï¼‰

## æ•°æ®å‡†å¤‡æ­¥éª¤
1. æ”¶é›†ç”µæ¢¯æŒ‰é’®å›¾ç‰‡
2. ä½¿ç”¨æ ‡æ³¨å·¥å…·ï¼ˆå¦‚ labelImgã€Roboflowï¼‰è¿›è¡Œæ ‡æ³¨
3. å°†æ ‡æ³¨ç»“æœè½¬æ¢ä¸ºYOLOæ ¼å¼
4. æŒ‰ç…§ä¸Šè¿°ç›®å½•ç»“æ„æ”¾ç½®æ–‡ä»¶
5. è¿è¡Œè®­ç»ƒè„šæœ¬

## è·å–æ•°æ®é›†
ç”±äºç‰ˆæƒåŸå› ï¼Œéœ€è¦è‡ªè¡Œæ”¶é›†å’Œæ ‡æ³¨ç”µæ¢¯æŒ‰é’®å›¾ç‰‡ï¼Œæˆ–ä½¿ç”¨å…¬å¼€æ•°æ®é›†ã€‚

é›Œå°é¬¼å»ºè®®ï¼š
- æ‹æ‘„ä¸åŒç±»å‹çš„ç”µæ¢¯æŒ‰é’®
- åŒ…å«å„ç§å…‰ç…§æ¡ä»¶
- åŒ…å«ä¸åŒè§’åº¦å’Œè·ç¦»
- æ ‡æ³¨è¦å‡†ç¡®ï¼Œè¾¹ç•Œæ¡†è´´åˆæŒ‰é’®
"""
    
    readme_path = os.path.join(base_dir, "README.md")
    with open(readme_path, 'w', encoding='utf-8') as f:
        f.write(readme_content)
    
    print(f"ğŸ“ è¯´æ˜æ–‡ä»¶å·²åˆ›å»º: {readme_path}")


def train_elevator_model(
    data_config: str,
    model_size: str = "yolov8n",
    epochs: int = 100,
    batch_size: int = 16,
    imgsz: int = 640,
    device: str = "auto",
    project: str = "elevator_detection",
    name: str = "train"
) -> None:
    """
    è®­ç»ƒç”µæ¢¯æŒ‰é’®æ£€æµ‹æ¨¡å‹
    
    Args:
        data_config: æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„
        model_size: æ¨¡å‹å¤§å° (yolov8n, yolov8s, yolov8m, yolov8l, yolov8x)
        epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹é‡å¤§å°
        imgsz: è¾“å…¥å›¾åƒå°ºå¯¸
        device: è®¾å¤‡ (auto, cpu, cuda)
        project: é¡¹ç›®åç§°
        name: å®éªŒåç§°
    """
    print("ğŸ€ é›Œå°é¬¼å¼€å§‹è®­ç»ƒç”µæ¢¯æŒ‰é’®æ£€æµ‹æ¨¡å‹...")
    print("=" * 60)
    print(f"ğŸ“Š è®­ç»ƒå‚æ•°:")
    print(f"   æ•°æ®é…ç½®: {data_config}")
    print(f"   æ¨¡å‹å¤§å°: {model_size}")
    print(f"   è®­ç»ƒè½®æ•°: {epochs}")
    print(f"   æ‰¹é‡å¤§å°: {batch_size}")
    print(f"   å›¾åƒå°ºå¯¸: {imgsz}")
    print(f"   è®¾å¤‡: {device}")
    print("=" * 60)
    
    try:
        # æ£€æŸ¥æ•°æ®é›†é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(data_config):
            raise FileNotFoundError(f"æ•°æ®é›†é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {data_config}")
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        model = YOLO(f'{model_size}.pt')
        
        # å¼€å§‹è®­ç»ƒ
        results = model.train(
            data=data_config,
            epochs=epochs,
            batch=batch_size,
            imgsz=imgsz,
            device=device,
            project=project,
            name=name,
            save=True,
            plots=True,
            verbose=True
        )
        
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: {project}/{name}/weights/")
        print("ğŸ’¡ ä½¿ç”¨ best.pt æ–‡ä»¶è¿›è¡Œæ¨ç†")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        print("ğŸ’¡ è¯·æ£€æŸ¥æ•°æ®é›†æ ¼å¼å’Œè·¯å¾„æ˜¯å¦æ­£ç¡®")


def validate_dataset(data_config: str) -> bool:
    """
    éªŒè¯æ•°æ®é›†æ ¼å¼å’Œå®Œæ•´æ€§
    
    Args:
        data_config: æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        æ˜¯å¦éªŒè¯é€šè¿‡
    """
    print("ğŸ” éªŒè¯æ•°æ®é›†...")
    
    try:
        # è¯»å–é…ç½®æ–‡ä»¶
        with open(data_config, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        base_path = config.get('path', '')
        train_path = os.path.join(base_path, config.get('train', ''))
        val_path = os.path.join(base_path, config.get('val', ''))
        
        # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(train_path):
            print(f"âŒ è®­ç»ƒå›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: {train_path}")
            return False
            
        if not os.path.exists(val_path):
            print(f"âŒ éªŒè¯å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: {val_path}")
            return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å›¾ç‰‡æ–‡ä»¶
        train_images = list(Path(train_path).glob('*.jpg')) + list(Path(train_path).glob('*.png'))
        val_images = list(Path(val_path).glob('*.jpg')) + list(Path(val_path).glob('*.png'))
        
        if len(train_images) == 0:
            print(f"âŒ è®­ç»ƒé›†ä¸­æ²¡æœ‰å›¾ç‰‡æ–‡ä»¶")
            return False
            
        if len(val_images) == 0:
            print(f"âŒ éªŒè¯é›†ä¸­æ²¡æœ‰å›¾ç‰‡æ–‡ä»¶")
            return False
        
        print(f"âœ… æ•°æ®é›†éªŒè¯é€šè¿‡")
        print(f"   è®­ç»ƒå›¾ç‰‡: {len(train_images)} å¼ ")
        print(f"   éªŒè¯å›¾ç‰‡: {len(val_images)} å¼ ")
        print(f"   ç±»åˆ«æ•°é‡: {config.get('nc', 0)}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®é›†éªŒè¯å¤±è´¥: {e}")
        return False


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    parser = argparse.ArgumentParser(
        description="ğŸ€ é›Œå°é¬¼çš„ç”µæ¢¯æŒ‰é’®æ£€æµ‹è®­ç»ƒå·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
    # åˆ›å»ºæ•°æ®é›†ç»“æ„
    python train_elevator.py --create-dataset ./elevator_dataset
    
    # éªŒè¯æ•°æ®é›†
    python train_elevator.py --validate ./elevator_dataset/elevator_data.yaml
    
    # å¼€å§‹è®­ç»ƒ
    python train_elevator.py --train ./elevator_dataset/elevator_data.yaml
    
    # è‡ªå®šä¹‰è®­ç»ƒå‚æ•°
    python train_elevator.py --train ./elevator_dataset/elevator_data.yaml --model yolov8s --epochs 200
        """
    )
    
    # æ“ä½œæ¨¡å¼
    parser.add_argument('--create-dataset', type=str, help='åˆ›å»ºæ•°æ®é›†ç›®å½•ç»“æ„')
    parser.add_argument('--validate', type=str, help='éªŒè¯æ•°æ®é›†é…ç½®æ–‡ä»¶')
    parser.add_argument('--train', type=str, help='è®­ç»ƒæ¨¡å‹ï¼ˆæŒ‡å®šæ•°æ®é›†é…ç½®æ–‡ä»¶ï¼‰')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--model', type=str, default='yolov8n', 
                       choices=['yolov8n', 'yolov8s', 'yolov8m', 'yolov8l', 'yolov8x'],
                       help='æ¨¡å‹å¤§å° (é»˜è®¤: yolov8n)')
    parser.add_argument('--epochs', type=int, default=100, help='è®­ç»ƒè½®æ•° (é»˜è®¤: 100)')
    parser.add_argument('--batch', type=int, default=16, help='æ‰¹é‡å¤§å° (é»˜è®¤: 16)')
    parser.add_argument('--imgsz', type=int, default=640, help='å›¾åƒå°ºå¯¸ (é»˜è®¤: 640)')
    parser.add_argument('--device', type=str, default='auto', help='è®¾å¤‡ (é»˜è®¤: auto)')
    parser.add_argument('--project', type=str, default='elevator_detection', help='é¡¹ç›®åç§°')
    parser.add_argument('--name', type=str, default='train', help='å®éªŒåç§°')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ“ä½œæ¨¡å¼
    if args.create_dataset:
        print("ğŸ€ åˆ›å»ºæ•°æ®é›†ç›®å½•ç»“æ„...")
        create_dataset_structure(args.create_dataset)
        create_dataset_config(args.create_dataset)
        print("\nğŸ’¡ æ¥ä¸‹æ¥è¯·:")
        print("   1. å°†ç”µæ¢¯æŒ‰é’®å›¾ç‰‡æ”¾å…¥å¯¹åº”çš„imagesæ–‡ä»¶å¤¹")
        print("   2. ä½¿ç”¨æ ‡æ³¨å·¥å…·åˆ›å»ºYOLOæ ¼å¼çš„æ ‡ç­¾æ–‡ä»¶")
        print("   3. è¿è¡ŒéªŒè¯å‘½ä»¤æ£€æŸ¥æ•°æ®é›†")
        print("   4. å¼€å§‹è®­ç»ƒæ¨¡å‹")
        
    elif args.validate:
        validate_dataset(args.validate)
        
    elif args.train:
        if validate_dataset(args.train):
            train_elevator_model(
                data_config=args.train,
                model_size=args.model,
                epochs=args.epochs,
                batch_size=args.batch,
                imgsz=args.imgsz,
                device=args.device,
                project=args.project,
                name=args.name
            )
        else:
            print("âŒ æ•°æ®é›†éªŒè¯å¤±è´¥ï¼Œè¯·ä¿®å¤åå†è®­ç»ƒ")
    else:
        parser.print_help()
        print("\nğŸ€ å¤§å§å§ï½è¯·é€‰æ‹©ä¸€ä¸ªæ“ä½œæ¨¡å¼ï¼")


if __name__ == "__main__":
    main()
